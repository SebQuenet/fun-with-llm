# Fun with LLM

A comprehensive TypeScript project for experimenting with Large Language Models, featuring multi-LLM benchmarking, prompt chaining, schema validation with Zod, and testing with Vitest.

## ğŸ¯ Use Cases

### 1. Prompt Chaining
Demonstrates chaining multiple prompts together, where the output of one becomes the input for the next.

### 2. LLM Benchmark System
A comprehensive AI benchmarking system that evaluates and compares multiple language models:

#### ğŸ”§ **Core Workflow**
1. **Question Generation**: Uses OpenAI GPT-4o mini to generate challenging benchmark questions
2. **Multi-LLM Evaluation**: Sends questions to all configured LLMs for analysis
3. **Intelligent Judging**: Uses GPT-4o mini to evaluate responses and create ranked leaderboards

#### ğŸ¤– **Supported LLMs**
- **OpenAI GPT-4o mini** (via OpenAI SDK)
- **Claude 3.5 Sonnet** (via Anthropic API)
- **Llama 3.2** (via Together.ai API)  
- **Deepseek V3** (via Deepseek API)

#### âœ¨ **Features**
- **Automated Question Generation**: Creates intellectually demanding questions testing reasoning, creativity, and knowledge synthesis
- **Comprehensive Analysis**: Each LLM provides detailed analysis demonstrating their capabilities
- **AI-Powered Judging**: Evaluates responses across multiple criteria (accuracy, depth, creativity, clarity)
- **Detailed Leaderboards**: JSON output with scores, strengths, weaknesses, and rationale for each LLM
- **Agent Factory**: Easy instantiation with flexible configuration
- **Mock Testing**: Comprehensive test coverage with mock agents
- **Edge Case Handling**: Robust error handling and boundary testing

#### ğŸ“Š **Evaluation Criteria**
- Accuracy and factual correctness
- Depth of analysis and reasoning  
- Creativity and innovation in approach
- Completeness of response
- Clarity and structure
- Practical applicability
- Handling of complexity and nuance
- Evidence of critical thinking

## Features

- ğŸ¤– Multi-LLM integration with 4 major providers
- ğŸ† Automated benchmarking and evaluation system
- ğŸ” Schema validation using Zod
- âš¡ Fast testing with Vitest
- ğŸ“ TypeScript for type safety
- ğŸ”„ Hot reload development with tsx
- ğŸ§ª Comprehensive test coverage (160+ tests)

## Prerequisites

- Node.js (v18 or higher)
- API keys for desired LLM providers:
  - OpenAI API key (required for benchmarking)
  - Anthropic API key (for Claude)
  - Together API key (for Llama)
  - Deepseek API key (for Deepseek)

## Setup

1. **Clone and install dependencies:**
   ```bash
   npm install
   ```

2. **Set up environment variables:**
   Create a `.env` file in the root directory:
   ```env
   OPENAI_API_KEY=your_openai_api_key_here
   ANTHROPIC_API_KEY=your_anthropic_api_key_here
   TOGETHER_API_KEY=your_together_api_key_here
   DEEPSEEK_API_KEY=your_deepseek_api_key_here
   ```

## Usage

### Run LLM Benchmark Demo
```bash
# Execute a full benchmark comparison
npx tsx src/useCases/llmBenchmark/demo.ts
```

### Run the interactive chat
```bash
npm start
```

This starts an interactive prompt where you can chat with GPT-4o-mini.

### Development commands
```bash
# Build the project
npm run build

# Type checking
npm run lint

# Watch mode compilation
npm run dev
```

### Testing
```bash
# Run tests in watch mode
npm test

# Run tests once
npm run test:run

# Run specific test suites
npm test -- edge-cases.test.ts
npm test -- new-benchmark-workflow.test.ts
```

## Project Structure

```
src/
â”œâ”€â”€ index.ts                           # Main application entry point
â”œâ”€â”€ useCases/
â”‚   â”œâ”€â”€ prompt-chaining/              # Prompt chaining use case
â”‚   â”‚   â”œâ”€â”€ prompt-chaining.usecase.ts
â”‚   â”‚   â””â”€â”€ __test__/
â”‚   â””â”€â”€ llmBenchmark/                 # LLM benchmarking system
â”‚       â”œâ”€â”€ llm-benchmark.usecase.ts  # Main benchmark orchestrator
â”‚       â”œâ”€â”€ demo.ts                   # Benchmark demo runner
â”‚       â”œâ”€â”€ agents/                   # LLM agent implementations
â”‚       â”‚   â”œâ”€â”€ Agent.ts              # Agent interface
â”‚       â”‚   â”œâ”€â”€ AgentFactory.ts       # Agent factory
â”‚       â”‚   â”œâ”€â”€ OpenAIAgent.ts        # OpenAI implementation
â”‚       â”‚   â”œâ”€â”€ ClaudeAgent.ts        # Claude implementation
â”‚       â”‚   â”œâ”€â”€ LlamaAgent.ts         # Llama implementation
â”‚       â”‚   â””â”€â”€ DeepseekAgent.ts      # Deepseek implementation
â”‚       â”œâ”€â”€ services/                 # Core services
â”‚       â”‚   â”œâ”€â”€ QuestionGenerator.ts  # Benchmark question generator
â”‚       â”‚   â””â”€â”€ ResponseJudge.ts      # Response evaluation service
â”‚       â””â”€â”€ __test__/                 # Comprehensive test suite
â”‚           â”œâ”€â”€ agents/               # Individual agent tests
â”‚           â”œâ”€â”€ mocks/                # Mock implementations
â”‚           â”œâ”€â”€ utils/                # Test utilities
â”‚           â”œâ”€â”€ new-benchmark-workflow.test.ts
â”‚           â”œâ”€â”€ agent-factory.test.ts
â”‚           â”œâ”€â”€ edge-cases.test.ts
â”‚           â””â”€â”€ benchmarking-scenarios.test.ts
â””â”€â”€ CLAUDE.md                        # Development context file
```

## Example Usage

### Basic LLM Benchmark
```typescript
import { llmBenchmarkUseCase } from './src/useCases/llmBenchmark/llm-benchmark.usecase';
import { AgentFactory } from './src/useCases/llmBenchmark/agents/AgentFactory';

// Create agents
const agents = [
  AgentFactory.createAgent({ type: 'openai', model: 'gpt-4o-mini' }),
  AgentFactory.createAgent({ type: 'claude', model: 'claude-3-5-sonnet-20241022' }),
  AgentFactory.createAgent({ type: 'llama', model: 'llama-3.2-90b-text-preview' }),
  AgentFactory.createAgent({ type: 'deepseek', model: 'deepseek-chat' })
];

// Run benchmark
const result = await llmBenchmarkUseCase({ agents });

// Access results
console.log('Question:', result.question);
console.log('Leaderboard:', result.leaderboard.rankings);
```

### Agent Factory Usage
```typescript
import { AgentFactory } from './src/useCases/llmBenchmark/agents/AgentFactory';

// Create individual agents
const openaiAgent = AgentFactory.createAgent({ 
  type: 'openai', 
  model: 'gpt-4o-mini',
  apiKey: 'custom-key'
});

// Create multiple agents
const agents = AgentFactory.createMultipleAgents([
  { type: 'openai', model: 'gpt-4o-mini' },
  { type: 'claude', model: 'claude-3-5-sonnet-20241022' }
]);

// Create benchmark set
const benchmarkAgents = AgentFactory.createBenchmarkAgents();
```

## Dependencies

### Runtime
- **zod** - Schema validation
- **openai** - OpenAI API client
- **dotenv** - Environment variable management

### Development
- **typescript** - Type-safe JavaScript
- **vitest** - Fast unit testing
- **tsx** - TypeScript execution engine
- **@types/node** - Node.js type definitions

## Testing

The project includes comprehensive testing with 160+ tests covering:
- Individual agent functionality
- Agent factory patterns
- Edge cases and error handling
- Benchmarking scenarios
- Integration workflows
- Performance testing

## License

ISC