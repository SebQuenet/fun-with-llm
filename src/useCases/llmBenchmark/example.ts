import { llmBenchmarkUseCase } from './llm-benchmark.usecase';
import { AgentFactory } from './agents/AgentFactory';
import dotenv from 'dotenv';

dotenv.config();

async function runBenchmarkExample() {
  try {
    console.log('ğŸš€ Starting LLM Benchmark Example...\n');

    // Create agents using the factory
    const agents = AgentFactory.createStandardBenchmarkAgents();
    
    console.log(`ğŸ“Š Created ${agents.length} agents:`);
    agents.forEach(agent => {
      console.log(`  - ${agent.name} (${agent.provider})`);
    });
    console.log();

    // Run the benchmark use case
    const result = await llmBenchmarkUseCase({ agents });
    
    console.log(`âœ… Benchmark initialized with ${result.totalAgents} agents\n`);

    // Test each agent with a simple prompt
    const testPrompt = "What is the capital of France?";
    console.log(`ğŸ§ª Testing all agents with prompt: "${testPrompt}"\n`);

    for (const agent of agents) {
      try {
        console.log(`ğŸ¤– Testing ${agent.name}...`);
        const response = await agent.call(testPrompt);
        console.log(`âœ… Response: ${response}\n`);
      } catch (error) {
        console.log(`âŒ Error with ${agent.name}: ${error}\n`);
      }
    }

    console.log('ğŸ‰ Benchmark example completed!');

  } catch (error) {
    console.error('âŒ Benchmark example failed:', error);
  }
}

if (import.meta.url === `file://${process.argv[1]}`) {
  runBenchmarkExample();
}

export { runBenchmarkExample };